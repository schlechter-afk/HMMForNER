{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install datasets\n",
    "!pip3 install transformers\n",
    "!pip3 install sentencepiece\n",
    "!pip3 install protobuf\n",
    "!pip3 install torch\n",
    "!pip3 install hindi-stemmer\n",
    "!pip3 install indic-nlp-library\n",
    "!pip3 install snowballstemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoConfig, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForTokenClassification, EarlyStoppingCallback, IntervalStrategy\n",
    "from datasets import ClassLabel, load_dataset, load_metric, DownloadMode\n",
    "import torch\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "from indicnlp import common\n",
    "from indicnlp import loader\n",
    "from indicnlp.morph import unsupervised_morph\n",
    "from indicnlp.normalize.indic_normalize import IndicNormalizerFactory\n",
    "from snowballstemmer import stemmer\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 25.7M/25.7M [00:04<00:00, 5.94MB/s]\n",
      "Generating train split: 507741 examples [00:36, 13957.96 examples/s]\n",
      "Generating test split: 847 examples [00:00, 13956.19 examples/s]\n",
      "Generating validation split: 2700 examples [00:00, 14277.62 examples/s]\n"
     ]
    }
   ],
   "source": [
    "lang='te'\n",
    "raw_datasets = load_dataset('ai4bharat/naamapadam', lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tokens', 'ner_tags']\n",
      "{'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "column_names = raw_datasets[\"train\"].column_names\n",
    "print(column_names)\n",
    "\n",
    "features = raw_datasets[\"train\"].features\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = raw_datasets['train']\n",
    "val_data = raw_datasets['validation']\n",
    "test_data = raw_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['స్థాపించబడిన', 'సాఫ్ట్వేర్'], 'ner_tags': [3, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_column_name = \"tokens\"\n",
    "label_column_name = \"ner_tags\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6}\n"
     ]
    }
   ],
   "source": [
    "label_list = features[label_column_name].feature.names\n",
    "\n",
    "label_to_id = {label_list[i]: features[label_column_name].feature.str2int( label_list[i] ) for i in range(len(label_list))}\n",
    "\n",
    "print(label_to_id)\n",
    "\n",
    "num_labels = len(label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing tokenization on our datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained('ai4bharat/indic-bert', num_labels=num_labels, finetuning_task='ner')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\")\n",
    "model = AutoModelForTokenClassification.from_pretrained('ai4bharat/indic-bert', num_labels=num_labels )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = \"max_length\"\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[text_column_name],\n",
    "        padding=padding,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        is_split_into_words=True,\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[label_column_name]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        \n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # Ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['tokens', 'ner_tags'],\n",
      "    num_rows: 507741\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = raw_datasets[\"train\"]\n",
    "train_dataset = train_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    load_from_cache_file=True,\n",
    "    desc=\"Running tokenizer on train dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on test dataset (num_proc=4): 100%|██████████| 847/847 [00:00<00:00, 3430.32 examples/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = raw_datasets[\"test\"]\n",
    "test_dataset = test_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    load_from_cache_file=True,\n",
    "    desc=\"Running tokenizer on test dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 507741\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset after tokenization\n",
      "Dataset({\n",
      "    features: ['tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 985787\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset after tokenization\")\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Normalization on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer_factory = IndicNormalizerFactory()\n",
    "normalizer = normalizer_factory.get_normalizer(\"te\",remove_nuktas=False)\n",
    "\n",
    "for entry in train_dataset:\n",
    "    tokens = entry['tokens']\n",
    "    normalized_tokens = [normalizer.normalize(token) for token in tokens]\n",
    "    entry['tokens'] = normalized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in test_dataset:\n",
    "    tokens = entry['tokens']\n",
    "    normalized_tokens = [normalizer.normalize(token) for token in tokens]\n",
    "    entry['tokens'] = normalized_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Morph Analyzer on tokens to see their root form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDIC_NLP_RESOURCES = r\"../../indic_nlp_resources-master/\"\n",
    "common.set_resources_path(INDIC_NLP_RESOURCES)\n",
    "loader.load()\n",
    "\n",
    "analyzer = unsupervised_morph.UnsupervisedMorphAnalyzer('te')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 10\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "sample_dataset = train_dataset.select(range(10))\n",
    "\n",
    "def morph_hindi_tokens(tokens_list):\n",
    "    stemmed_tokens = []\n",
    "    for tokens in tokens_list:\n",
    "        stemmed_tokens_per_list = []\n",
    "        for token in tokens:\n",
    "            stem = analyzer.morph_analyze(token)\n",
    "            stemmed_tokens_per_list.append(\" \".join([s[0] for s in stem]))\n",
    "        stemmed_tokens.append(stemmed_tokens_per_list)\n",
    "    return stemmed_tokens\n",
    "\n",
    "morphed_dataset = sample_dataset.map(\n",
    "    lambda example: {\n",
    "        'tokens': morph_hindi_tokens(example['tokens']),\n",
    "        'ner_tags': example['ner_tags'],\n",
    "        'input_ids': example['input_ids'],\n",
    "        'token_type_ids': example['token_type_ids'],\n",
    "        'attention_mask': example['attention_mask'],\n",
    "        'labels': example['labels'],\n",
    "    },\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "print(morphed_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing stemming on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'tokens': train_dataset['tokens'],\n",
    "    'ner_tags': train_dataset['ner_tags'],\n",
    "    'input_ids': train_dataset['input_ids'],\n",
    "    'token_type_ids': train_dataset['token_type_ids'],\n",
    "    'attention_mask': train_dataset['attention_mask'],\n",
    "    'labels': train_dataset['labels'],\n",
    "})\n",
    "\n",
    "stemmed_dataset = {\n",
    "    'tokens': df['tokens'],\n",
    "    'ner_tags': df['ner_tags'],\n",
    "    'input_ids': df['input_ids'],\n",
    "    'token_type_ids': df['token_type_ids'],\n",
    "    'attention_mask': df['attention_mask'],\n",
    "    'labels': df['labels'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removal of noisy words from input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_characters = ['nn', 'n', '।', '/', '`', '+', '\\\\', '\"', '?', '▁(', '$', '@', '[', '_', \"'\", '!', ',', ':', '^', '|', ']', '=', '%', '&', '.', ')', '(', '#', '*', '', ';', '-', '}', '|', '\"']\n",
    "\n",
    "pattern = '|'.join(re.escape(char) for char in special_characters)\n",
    "pattern = f\"[{pattern}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_telugu_tokenized_array(tokenized_array):\n",
    "    cleaned_tokens = []\n",
    "    for token in tokenized_array:\n",
    "        token = re.sub(r'<[^>]+>', '', token) \n",
    "        token = re.sub(pattern, '', token) \n",
    "        token = ' '.join(token.split())\n",
    "        cleaned_tokens.append(token)\n",
    "\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data_test = []\n",
    "\n",
    "for line in test_dataset:\n",
    "    cleaned_line = clean_telugu_tokenized_array(line['tokens'])\n",
    "    line['tokens'] = cleaned_line\n",
    "    cleaned_data_test.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = []\n",
    "\n",
    "for line in train_dataset:\n",
    "    cleaned_line = clean_telugu_tokenized_array(line['tokens'])\n",
    "    line['tokens'] = cleaned_line\n",
    "    cleaned_data.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the clean data and test data as csv files\n",
    "df = pd.DataFrame(cleaned_data)\n",
    "df.to_csv('telugu_train_data.csv', index=False)\n",
    "\n",
    "df = pd.DataFrame(cleaned_data_test)\n",
    "df.to_csv('telugu_test_data.csv', index=False)\n",
    "\n",
    "df = pd.DataFrame(val_data)\n",
    "df.to_csv('telugu_val_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
