{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM:\n",
    "    def __init__(self, states, observations):\n",
    "        self.states = states\n",
    "        self.observations = observations\n",
    "        self.start_probabilities = np.zeros(len(states))\n",
    "        self.transition_probabilities = np.zeros((len(states), len(states)))\n",
    "        self.emission_probabilities = np.zeros((len(states), len(observations)))\n",
    "\n",
    "        self.state_index = {state: i for i, state in enumerate(states)}\n",
    "        self.observation_index = {obs: i for i, obs in enumerate(observations)}\n",
    "\n",
    "    def compute_start_probabilities(self, sequences):\n",
    "        for sequence in sequences:\n",
    "            start_state = sequence[0][1]  # The tag of the first token\n",
    "            self.start_probabilities[self.state_index[start_state]] += 1\n",
    "        self.start_probabilities /= np.sum(self.start_probabilities)\n",
    "\n",
    "    def compute_transition_probabilities(self, sequences):\n",
    "        for sequence in sequences:\n",
    "            for i in range(len(sequence) - 1):\n",
    "                current_state = sequence[i][1]\n",
    "                next_state = sequence[i + 1][1]\n",
    "                self.transition_probabilities[self.state_index[current_state], self.state_index[next_state]] += 1\n",
    "\n",
    "        for i in range(len(self.states)):\n",
    "            if(np.sum(self.transition_probabilities[i])==0):\n",
    "                continue\n",
    "            self.transition_probabilities[i] /= np.sum(self.transition_probabilities[i])\n",
    "\n",
    "    def compute_emission_probabilities(self, sequences):\n",
    "        for sequence in sequences:\n",
    "            for token, state in sequence:\n",
    "                self.emission_probabilities[self.state_index[state], self.observation_index[token]] += 1\n",
    "\n",
    "        for i in range(len(self.states)):\n",
    "            if(np.sum(self.emission_probabilities[i])==0):\n",
    "                continue\n",
    "            self.emission_probabilities[i] /= np.sum(self.emission_probabilities[i])\n",
    "\n",
    "    def viterbi_algorithm(self, obs):\n",
    "        viterbi_table = [[0.0 for _ in range(len(self.states))] for _ in range(len(obs))]\n",
    "        backpointer = [[0 for _ in range(len(self.states))] for _ in range(len(obs))]\n",
    "\n",
    "        for t in range(len(obs)):\n",
    "            for s in range(len(self.states)):\n",
    "                if t == 0:\n",
    "                    viterbi_table[t][s] = self.start_probabilities[s] * self.emission_probabilities[s][obs[t]]\n",
    "                else:\n",
    "                    max_prob = -1\n",
    "                    max_backpointer = -1\n",
    "\n",
    "                    for s_prime in range(len(self.states)):\n",
    "                        prob = viterbi_table[t-1][s_prime] * self.transition_probabilities[s_prime][s] * self.emission_probabilities[s][obs[t]]\n",
    "                        if prob > max_prob:\n",
    "                            max_prob = prob\n",
    "                            max_backpointer = s_prime\n",
    "\n",
    "                    viterbi_table[t][s] = max_prob\n",
    "                    backpointer[t][s] = max_backpointer\n",
    "\n",
    "        best_path_prob = max(viterbi_table[-1])\n",
    "        best_path_pointer = max(range(len(self.states)), key=lambda s: viterbi_table[-1][s])\n",
    "        best_path = [best_path_pointer]\n",
    "        for t in range(len(obs)-1, 0, -1):\n",
    "            best_path.insert(0, backpointer[t][best_path[0]])\n",
    "\n",
    "        return best_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_data(sequences, threshold):\n",
    "    word_counts = defaultdict(int)\n",
    "    for sequence in sequences:\n",
    "        for token, state in sequence:\n",
    "            word_counts[token] += 1\n",
    "\n",
    "    unk_sequences = []\n",
    "    for sequence in sequences:\n",
    "        unk_sequence = [(token if word_counts[token] > threshold else 'UNK', state) for token, state in sequence]\n",
    "        unk_sequences.append(unk_sequence)\n",
    "\n",
    "    return unk_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./telugu_train_data.csv')\n",
    "test = pd.read_csv('./telugu_test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./telugu_train_data.csv', converters={'tokens': ast.literal_eval, 'ner_tags': ast.literal_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [(row['tokens'], row['ner_tags']) for _, row in df_train.iterrows()]\n",
    "unique_tags = set(tag for _, tags in train_data for tag in tags)\n",
    "states = list(unique_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hmm_data = [list(zip(tokens, tags)) for tokens, tags in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_sequences = threshold_data(train_hmm_data, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('స్థాపించబడిన', 3), ('సాఫ్ట్వేర్', 0)]\n"
     ]
    }
   ],
   "source": [
    "print(unk_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens = set()\n",
    "for sequence in unk_sequences:\n",
    "    for token, _ in sequence:\n",
    "        unique_tokens.add(token)\n",
    "\n",
    "unique_tokens_list = list(unique_tokens)\n",
    "num_observations = len(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hmm(hmm_model, sequences):\n",
    "    hmm_model.compute_start_probabilities(sequences)\n",
    "    hmm_model.compute_transition_probabilities(sequences)\n",
    "    hmm_model.compute_emission_probabilities(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Probabilities:\n",
      "[0.66402556 0.11883815 0.         0.11047168 0.         0.10666462\n",
      " 0.        ]\n",
      "Transition Probabilities:\n",
      "[[8.41370570e-01 8.06962244e-02 0.00000000e+00 3.72564422e-02\n",
      "  0.00000000e+00 4.06767635e-02 0.00000000e+00]\n",
      " [3.47151738e-01 1.29405527e-02 6.33020841e-01 3.04723506e-03\n",
      "  2.68016850e-04 3.51917777e-03 5.24380794e-05]\n",
      " [7.06447729e-01 1.72742200e-02 2.57099070e-01 8.99470899e-03\n",
      "  1.44499179e-03 8.52764094e-03 2.11640212e-04]\n",
      " [5.43750488e-01 4.31287881e-03 1.21218427e-03 8.17834196e-03\n",
      "  4.39929246e-01 2.49720365e-03 1.19657675e-04]\n",
      " [3.99908806e-01 6.24505792e-03 1.37945364e-03 1.54221763e-02\n",
      "  5.71064950e-01 5.84680561e-03 1.32750769e-04]\n",
      " [8.02106899e-01 5.99001005e-03 7.97017866e-04 1.46829503e-02\n",
      "  2.25244180e-03 3.33014856e-02 1.40869195e-01]\n",
      " [5.87684336e-01 5.05679554e-03 9.21681945e-04 7.09944201e-03\n",
      "  1.84336389e-03 3.41520526e-02 3.63242328e-01]]\n",
      "Emission Probabilities:\n",
      "[[1.80569578e-06 1.19691835e-04 1.03182616e-06 ... 2.57956541e-07\n",
      "  3.61139157e-05 8.20301799e-05]\n",
      " [0.00000000e+00 2.58738907e-05 3.10486688e-04 ... 4.02482744e-05\n",
      "  2.87487674e-05 2.87487674e-06]\n",
      " [0.00000000e+00 0.00000000e+00 6.93433188e-06 ... 3.46716594e-06\n",
      "  3.81388253e-05 1.04014978e-05]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 5.41949610e-06 ... 5.41949610e-06\n",
      "  0.00000000e+00 1.08389922e-05]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "hmm_model = HMM(states, unique_tokens_list)\n",
    "\n",
    "train_hmm(hmm_model, unk_sequences)\n",
    "\n",
    "print(\"Start Probabilities:\")\n",
    "print(hmm_model.start_probabilities)\n",
    "print(\"Transition Probabilities:\")\n",
    "print(hmm_model.transition_probabilities)\n",
    "print(\"Emission Probabilities:\")\n",
    "print(hmm_model.emission_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9042406084351233\n",
      "UNK Accuracy: 0.7178051511758119\n"
     ]
    }
   ],
   "source": [
    "test_tokens = test['tokens']\n",
    "test_ner_tags = test['ner_tags']\n",
    "misvals = []\n",
    "\n",
    "N_test = len(test_tokens)\n",
    "\n",
    "def compute_accuracy(hmm_model):\n",
    "    count_correct_new = 0\n",
    "    tot_new = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for idx in range(N_test):\n",
    "        tags = eval(test_ner_tags[idx])\n",
    "        observations = [obs for obs in eval(test_tokens[idx])]\n",
    "        if(len(observations)==0):\n",
    "            continue\n",
    "        observed_tokens = [tok for tok in eval(test_ner_tags[idx])]\n",
    "        observation_indices = [hmm_model.observation_index[obs] if obs in hmm_model.observation_index else hmm_model.observation_index['UNK'] for obs in observations]\n",
    "        predicted_tags = hmm_model.viterbi_algorithm(observation_indices)\n",
    "        for i,obs in enumerate(observations):\n",
    "            if obs not in hmm_model.observation_index:\n",
    "                tot_new += 1 \n",
    "                if(predicted_tags[i] == tags[i]):\n",
    "                    count_correct_new += 1\n",
    "\n",
    "        for i in range(len(predicted_tags)):\n",
    "            if(predicted_tags[i] == observed_tokens[i]):\n",
    "                correct += 1\n",
    "        total += len(observations)\n",
    "    return correct / total, count_correct_new / tot_new\n",
    "\n",
    "\n",
    "accuracy, unk_accuracy = compute_accuracy(hmm_model)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"UNK Accuracy:\", unk_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
